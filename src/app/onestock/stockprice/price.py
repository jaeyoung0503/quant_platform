# Ï¢ÖÎ™©Î≥Ñ Î∞±ÌÖåÏä§Ìä∏ ÏóîÏßÑ
#file: stockprice/price.py

import asyncio
import pandas as pd
from datetime import datetime, timedelta
import redis
from typing import Dict, List, Optional
import logging
from dataclasses import dataclass
import time
import json
import yfinance as yf
import FinanceDataReader as fdr
from sqlalchemy import create_engine, Column, String, Float, DateTime, Integer
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os
from dotenv import load_dotenv

# ÌôòÍ≤ΩÎ≥ÄÏàò Î°úÎìú
load_dotenv()

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('quant_system.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

Base = declarative_base()

@dataclass
class StockPrice:
    """Ï£ºÍ∞Ä Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞"""
    symbol: str
    timestamp: datetime
    open: float
    high: float
    low: float
    close: float
    volume: int
    adj_close: Optional[float] = None
    currency: str = 'KRW'

class StockPriceDB(Base):
    """SQLAlchemy ORM Î™®Îç∏"""
    __tablename__ = 'stock_prices'
    
    id = Column(Integer, primary_key=True)
    symbol = Column(String(10), nullable=False, index=True)
    timestamp = Column(DateTime, nullable=False, index=True)
    open = Column(Float)
    high = Column(Float)
    low = Column(Float)
    close = Column(Float)
    volume = Column(Integer)
    adj_close = Column(Float)
    currency = Column(String(3))

class CompanyListManager:
    """Ï¢ÖÎ™©ÏΩîÎìú Í¥ÄÎ¶¨ Ï†ÑÏö© ÌÅ¥ÎûòÏä§"""
    
    def __init__(self, cache_manager=None, force_update: bool = False):
        self.cache_manager = cache_manager
        self.cache_key = "company_list:krx"
        self.cache_timeout = 86400  # 24ÏãúÍ∞Ñ
        self.companies_df = None
        self.symbols = []
        self.csv_file = "company_list.csv"
    
    async def load_companies_from_cache_or_download(self, force_update: bool = False):
        """Ï∫êÏãú ÎòêÎäî CSVÏóêÏÑú Ï¢ÖÎ™©ÏΩîÎìú Î°úÎìú, ÏóÜÏúºÎ©¥ Îã§Ïö¥Î°úÎìú"""
        try:
            if force_update:
                await self.download_and_cache_companies()
                return

            if self.cache_manager and self.cache_manager.redis_client:
                cached_data = await self.get_cached_companies()
                if cached_data is not None:
                    self.companies_df = cached_data
                    self._set_symbols_from_df()
                    logger.info(f"‚úì Loaded {len(self.symbols)} companies from Redis cache")
                    return
            
            if os.path.exists(self.csv_file):
                logger.info("Loading companies from CSV file...")
                self.companies_df = pd.read_csv(self.csv_file, dtype={'Code': str})
                self._set_symbols_from_df()
                logger.info(f"‚úì Loaded {len(self.symbols)} companies from CSV")
                if self.cache_manager:
                    await self.cache_companies(self.companies_df)
            else:
                logger.info("No cached data found, downloading fresh company list...")
                await self.download_and_cache_companies()
                
        except Exception as e:
            logger.error(f"Error loading company list: {e}")
            self.symbols = []
            self.companies_df = pd.DataFrame()

    def _set_symbols_from_df(self):
        """DataFrameÏóêÏÑú Ï¢ÖÎ™©ÏΩîÎìú Î¶¨Ïä§Ìä∏ ÏÑ§Ï†ï"""
        if self.companies_df is None or 'Code' not in self.companies_df.columns:
            logger.error("DataFrame is not valid or 'Code' column is missing.")
            self.symbols = []
            return
        self.companies_df['Code'] = self.companies_df['Code'].astype(str).str.zfill(6)
        self.symbols = self.companies_df['Code'].tolist()

    async def download_and_cache_companies(self):
        """Ï¢ÖÎ™©ÏΩîÎìú Îã§Ïö¥Î°úÎìú, CSV Ï†ÄÏû• Î∞è Ï∫êÏã±"""
        try:
            logger.info("üì° Downloading KRX company list...")
            krx_list = fdr.StockListing('KRX')
            
            required_cols = ['Code', 'Name', 'Market']
            if not all(col in krx_list.columns for col in required_cols):
                 # 'Symbol' Ïª¨ÎüºÏùÑ 'Code'Î°ú Î≥ÄÍ≤Ω
                if 'Symbol' in krx_list.columns and 'Code' not in krx_list.columns:
                    krx_list.rename(columns={'Symbol': 'Code'}, inplace=True)
                else:
                    raise ValueError(f"Required columns {required_cols} not found in KRX data.")
            
            cleaned_list = krx_list[required_cols + ['Sector']].copy()
            cleaned_list = cleaned_list.dropna(subset=['Code'])
            
            self.companies_df = cleaned_list
            self._set_symbols_from_df()
            
            cleaned_list.to_csv(self.csv_file, index=False, encoding='utf-8-sig')
            logger.info(f"üíæ Company list saved to CSV: {self.csv_file}")
            
            if self.cache_manager:
                await self.cache_companies(cleaned_list)
            
            logger.info(f"‚úÖ Downloaded and cached {len(self.symbols)} companies")
            
        except Exception as e:
            logger.error(f"‚ùå Error downloading company list: {e}")
            self.symbols = []
            self.companies_df = pd.DataFrame()

    async def get_cached_companies(self) -> Optional[pd.DataFrame]:
        """Ï∫êÏãúÏóêÏÑú Ï¢ÖÎ™©ÏΩîÎìú Ï°∞Ìöå"""
        if not self.cache_manager or not self.cache_manager.redis_client: return None
        try:
            cached = self.cache_manager.redis_client.get(self.cache_key)
            if cached:
                return pd.DataFrame(json.loads(cached))
            return None
        except Exception as e:
            logger.warning(f"Cache get error for companies: {e}")
            return None

    async def cache_companies(self, df: pd.DataFrame):
        """Ï¢ÖÎ™©ÏΩîÎìúÎ•º Ï∫êÏãúÏóê Ï†ÄÏû•"""
        if not self.cache_manager or not self.cache_manager.redis_client: return
        try:
            data = df.to_dict('records')
            self.cache_manager.redis_client.setex(
                self.cache_key, self.cache_timeout, json.dumps(data)
            )
            logger.info(f"üìù Company list cached for {self.cache_timeout/3600:.1f} hours")
        except Exception as e:
            logger.warning(f"Cache set error for companies: {e}")

    def get_test_symbols(self, count: int = 10) -> List[str]:
        """ÌÖåÏä§Ìä∏Ïö© Ï¢ÖÎ™© Î∞òÌôò (ÏãúÍ∞ÄÏ¥ùÏï° ÏÉÅÏúÑ)"""
        if self.companies_df is None or self.companies_df.empty:
            logger.warning("No company data available, returning default symbols.")
            return ['005930', '000660', '035420', '005935', '005490', 
                   '051910', '006400', '035720', '105560', '055550'][:count]
        
        kospi_companies = self.companies_df[self.companies_df['Market'] == 'KOSPI']
        selected = kospi_companies.head(count) if len(kospi_companies) >= count else self.companies_df.head(count)
        
        test_symbols = selected['Code'].tolist()
        logger.info(f"üîç Selected {len(test_symbols)} test symbols: {test_symbols}")
        return test_symbols

class DataSourceManager:
    """Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§ Í¥ÄÎ¶¨"""
    
    def _validate_stock_data(self, symbol: str, row: pd.Series) -> bool:
        """Ï£ºÍ∞Ä Îç∞Ïù¥ÌÑ∞ Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨"""
        prices = [row['Open'], row['High'], row['Low'], row['Close']]
        if any(p < 0 for p in prices) or row['Volume'] < 0:
            logger.warning(f"Negative value detected for {symbol}: {row.to_dict()}")
            return False
        if row['High'] < row['Low']:
            logger.warning(f"Invalid high-low range for {symbol}: High={row['High']}, Low={row['Low']}")
            return False
        return True

    async def _get_yfinance_data(self, symbol: str, start_date: str, end_date: str) -> List[StockPrice]:
        """Yahoo Finance Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        try:
            symbol_yf = f"{symbol}.KS"
            ticker = yf.Ticker(symbol_yf)
            hist = ticker.history(start=start_date, end=end_date)
            
            if hist.empty: return []
            
            valid_data = []
            for idx, row in hist.iterrows():
                if self._validate_stock_data(symbol, row):
                    valid_data.append(StockPrice(
                        symbol=symbol, timestamp=idx.to_pydatetime(),
                        open=float(row['Open']), high=float(row['High']),
                        low=float(row['Low']), close=float(row['Close']),
                        volume=int(row['Volume']), adj_close=float(row.get('Adj Close', row['Close']))
                    ))
            return valid_data
        except Exception as e:
            logger.error(f"Error fetching yfinance data for {symbol}: {e}")
            return []

    async def _get_fdr_data(self, symbol: str, start_date: str, end_date: str) -> List[StockPrice]:
        """FinanceDataReader Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        try:
            df = fdr.DataReader(symbol, start_date, end_date)
            if df.empty: return []
            
            valid_data = []
            for idx, row in df.iterrows():
                if self._validate_stock_data(symbol, row):
                    valid_data.append(StockPrice(
                        symbol=symbol, timestamp=idx.to_pydatetime(),
                        open=float(row['Open']), high=float(row['High']),
                        low=float(row['Low']), close=float(row['Close']),
                        volume=int(row['Volume']), adj_close=float(row.get('Adj Close', row['Close']))
                    ))
            return valid_data
        except Exception as e:
            logger.error(f"Error fetching FDR data for {symbol}: {e}")
            return []

class CacheManager:
    """Redis Ï∫êÏãú Í¥ÄÎ¶¨"""
    def __init__(self, host='localhost', port=6379, db=0):
        try:
            self.redis_client = redis.Redis(host=host, port=port, db=db, decode_responses=True)
            self.redis_client.ping()
            self.cache_timeout = 300  # 5Î∂Ñ
            logger.info("‚úÖ Redis cache connected")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Redis not available: {e}")
            self.redis_client = None

    async def get_cached_data(self, key: str) -> Optional[List[StockPrice]]:
        if not self.redis_client: return None
        try:
            cached = self.redis_client.get(key)
            if cached:
                data = json.loads(cached)
                return [StockPrice(**item) for item in data]
            return None
        except Exception as e:
            logger.error(f"Cache get error: {e}")
            return None

    async def cache_data(self, key: str, data: List[StockPrice]):
        if not self.redis_client: return
        try:
            serialized = [item.__dict__ for item in data]
            for item in serialized:
                item['timestamp'] = item['timestamp'].isoformat()
            self.redis_client.setex(key, self.cache_timeout, json.dumps(serialized))
        except Exception as e:
            logger.error(f"Cache set error: {e}")

class DatabaseManager:
    """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Í¥ÄÎ¶¨"""
    def __init__(self, db_url: str = "sqlite:///stock_data.db"):
        self.engine = create_engine(db_url)
        Base.metadata.create_all(self.engine)
        self.Session = sessionmaker(bind=self.engine)
    
    async def save_stock_data(self, data: List[StockPrice]):
        session = self.Session()
        try:
            for item in data:
                db_item = StockPriceDB(**item.__dict__)
                session.merge(db_item) # Ï§ëÎ≥µ Ïãú ÏóÖÎç∞Ïù¥Ìä∏, ÏóÜÏúºÎ©¥ ÏÇΩÏûÖ
            session.commit()
        except Exception as e:
            session.rollback()
            logger.error(f"Database save error: {e}")
        finally:
            session.close()

class QuantDataManager:
    """ÌÜµÌï© Îç∞Ïù¥ÌÑ∞ Í¥ÄÎ¶¨ ÏãúÏä§ÌÖú"""
    def __init__(self, db_url: str = "sqlite:///quant_data.db", force_update_companies: bool = False):
        self.data_source = DataSourceManager()
        self.cache_manager = CacheManager()
        self.db_manager = DatabaseManager(db_url)
        self.company_manager = CompanyListManager(self.cache_manager, force_update_companies)
        self.csv_output_dir = "stock_data_csv"
        os.makedirs(self.csv_output_dir, exist_ok=True)
    
    async def initialize_companies(self, force_update: bool = False):
        await self.company_manager.load_companies_from_cache_or_download(force_update)

    def get_test_symbols(self, count: int = 10) -> List[str]:
        return self.company_manager.get_test_symbols(count)
    
    async def get_stock_data(self, symbol: str, start_date: str, end_date: str, use_cache: bool = True) -> List[StockPrice]:
        cache_key = f"stock_data:{symbol}:{start_date}:{end_date}"
        if use_cache and self.cache_manager.redis_client:
            cached_data = await self.cache_manager.get_cached_data(cache_key)
            if cached_data:
                logger.info(f"üìã Cache hit for {symbol}")
                return cached_data
        
        logger.info(f"üì° Fetching data for {symbol} from API...")
        data = await self.data_source._get_yfinance_data(symbol, start_date, end_date)
        
        if not data: # yfinance Ïã§Ìå® Ïãú FDR ÏãúÎèÑ
            logger.warning(f"yfinance failed for {symbol}. Trying FinanceDataReader...")
            await asyncio.sleep(1) # ÏßßÏùÄ ÎîúÎ†àÏù¥
            data = await self.data_source._get_fdr_data(symbol, start_date, end_date)

        if data:
            if self.cache_manager.redis_client:
                await self.cache_manager.cache_data(cache_key, data)
            await self.db_manager.save_stock_data(data)
            logger.info(f"‚úÖ Data fetched for {symbol}: {len(data)} records")
        return data
    
    async def collect_test_data(self, test_symbols: List[str], start_date: str, end_date: str) -> Dict[str, List[StockPrice]]:
        logger.info(f"üîÑ Starting data collection for {len(test_symbols)} symbols...")
        results = {}
        for i, symbol in enumerate(test_symbols, 1):
            try:
                logger.info(f"[{i}/{len(test_symbols)}] Processing {symbol}...")
                data = await self.get_stock_data(symbol, start_date, end_date)
                if data:
                    results[symbol] = data
                else:
                    logger.warning(f"  ‚ùå No data for {symbol}")
                await asyncio.sleep(1) # API ÏöîÏ≤≠ Í∞Ñ ÎîúÎ†àÏù¥
            except Exception as e:
                logger.error(f"  ‚ùå Error processing {symbol}: {e}")
        logger.info(f"üèÅ Test data collection completed: {sum(len(v) for v in results.values())} total records")
        return results

    async def save_data_to_csv(self, data_dict: Dict[str, List[StockPrice]], filename_prefix: str):
        if not data_dict:
            logger.warning("No data to save.")
            return None
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_filename = f"{self.csv_output_dir}/{filename_prefix}_{timestamp}.csv"
        
        all_records = []
        for symbol, stock_data in data_dict.items():
            company_name = self._get_company_name(symbol)
            for price in stock_data:
                record = price.__dict__
                record['company_name'] = company_name
                record['timestamp'] = price.timestamp.strftime('%Y-%m-%d %H:%M:%S')
                all_records.append(record)
        
        if not all_records:
            logger.warning("No valid records to save in CSV.")
            return None

        df_all = pd.DataFrame(all_records)
        df_all.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        logger.info(f"üíæ All data saved to CSV: {csv_filename}")
        return csv_filename

    def _get_company_name(self, symbol: str) -> str:
        """Ï¢ÖÎ™© ÏΩîÎìúÎ°ú ÌöåÏÇ¨ Ïù¥Î¶Ñ Ï°∞Ìöå"""
        if self.company_manager.companies_df is not None:
            info = self.company_manager.companies_df[self.company_manager.companies_df['Code'] == symbol]
            if not info.empty:
                return info.iloc[0]['Name']
        return "Unknown"

async def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò - ÏÇ¨Ïö©Ïûê ÏûÖÎ†• Í∏∞Î∞ò"""
    
    # 1. Ï¢ÖÎ™© Í∞úÏàò ÏûÖÎ†• Î∞õÍ∏∞
    while True:
        try:
            count_str = input("1. Îã§Ïö¥Î°úÎìúÌï† Ï¢ÖÎ™© Í∞ØÏàòÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî (Í∏∞Î≥∏Í∞í: 10): ")
            if not count_str:
                test_count = 10
                break
            test_count = int(count_str)
            if test_count > 0:
                break
            else:
                logger.warning("‚ùå 1 Ïù¥ÏÉÅÏùò Ïà´ÏûêÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.")
        except ValueError:
            logger.warning("‚ùå Ïú†Ìö®Ìïú Ïà´ÏûêÍ∞Ä ÏïÑÎãôÎãàÎã§.")

    # 2. Ïó∞ÎèÑ ÏûÖÎ†• Î∞õÍ∏∞
    while True:
        try:
            year_str = input(f"2. Îç∞Ïù¥ÌÑ∞Î•º Îã§Ïö¥Î°úÎìúÌï† Ïó∞ÎèÑÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî (YYYY, Ïòà: {datetime.now().year - 1}): ")
            year = int(year_str)
            if 1990 < year <= datetime.now().year:
                start_date = f"{year}-01-01"
                end_date = f"{year}-12-31"
                break
            else:
                logger.warning(f"‚ùå 1990ÎÖÑÍ≥º {datetime.now().year}ÎÖÑ ÏÇ¨Ïù¥Ïùò Ïú†Ìö®Ìïú Ïó∞ÎèÑÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî.")
        except ValueError:
            logger.warning("‚ùå Ïú†Ìö®Ìïú Ïó∞ÎèÑ ÌòïÏãùÏù¥ ÏïÑÎãôÎãàÎã§.")
            
    logger.info("üöÄ Initializing Quant Data Management System...")
    quant_manager = QuantDataManager()
    
    try:
        await quant_manager.initialize_companies()
        test_symbols = quant_manager.get_test_symbols(test_count)
        
        if not test_symbols:
            logger.error("No symbols to process. Exiting.")
            return
            
        collected_data = await quant_manager.collect_test_data(test_symbols, start_date, end_date)
        
        if collected_data:
            filename_prefix = f"stock_data_{len(test_symbols)}symbols_{year}"
            await quant_manager.save_data_to_csv(collected_data, filename_prefix)
        else:
            logger.warning("‚ö†Ô∏è No data collected.")
            
    except Exception as e:
        logger.error(f"üí• An unexpected error occurred during execution: {e}")
        import traceback
        traceback.print_exc()
    
    logger.info("üèÅ Quant Data Management System finished")

if __name__ == "__main__":
    asyncio.run(main())