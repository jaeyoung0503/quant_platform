# ì¢…ëª©ë³„ ë°±í…ŒìŠ¤íŠ¸ ì—”ì§„
#file: stockprice/price.py

import asyncio
import pandas as pd
from datetime import datetime, timedelta
import redis
from typing import Dict, List, Optional
import logging
from dataclasses import dataclass
import time
import json
import yfinance as yf
import FinanceDataReader as fdr
from sqlalchemy import create_engine, Column, String, Float, DateTime, Integer
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
import os
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('quant_system.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

Base = declarative_base()

@dataclass
class StockPrice:
    """ì£¼ê°€ ë°ì´í„° êµ¬ì¡°"""
    symbol: str
    timestamp: datetime
    open: float
    high: float
    low: float
    close: float
    volume: int
    adj_close: Optional[float] = None
    currency: str = 'KRW'

class StockPriceDB(Base):
    """SQLAlchemy ORM ëª¨ë¸"""
    __tablename__ = 'stock_prices'
    
    id = Column(Integer, primary_key=True)
    symbol = Column(String(10), nullable=False, index=True)
    timestamp = Column(DateTime, nullable=False, index=True)
    open = Column(Float)
    high = Column(Float)
    low = Column(Float)
    close = Column(Float)
    volume = Column(Integer)
    adj_close = Column(Float)
    currency = Column(String(3))

class CompanyListManager:
    """ì¢…ëª©ì½”ë“œ ê´€ë¦¬ ì „ìš© í´ë˜ìŠ¤"""
    
    def __init__(self, cache_manager=None, force_update: bool = False):
        self.cache_manager = cache_manager
        self.cache_key = "company_list:krx"
        self.cache_timeout = 86400  # 24ì‹œê°„
        self.companies_df = None
        self.symbols = []
        self.csv_file = "company_list.csv"
    
    async def load_companies_from_cache_or_download(self, force_update: bool = False):
        """ìºì‹œ ë˜ëŠ” CSVì—ì„œ ì¢…ëª©ì½”ë“œ ë¡œë“œ, ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ"""
        try:
            if force_update:
                await self.download_and_cache_companies()
                return

            if self.cache_manager and self.cache_manager.redis_client:
                cached_data = await self.get_cached_companies()
                if cached_data is not None:
                    self.companies_df = cached_data
                    self._set_symbols_from_df()
                    logger.info(f"âœ“ Loaded {len(self.symbols)} companies from Redis cache")
                    return
            
            if os.path.exists(self.csv_file):
                logger.info("Loading companies from CSV file...")
                self.companies_df = pd.read_csv(self.csv_file, dtype={'Code': str})
                self._set_symbols_from_df()
                logger.info(f"âœ“ Loaded {len(self.symbols)} companies from CSV")
                if self.cache_manager:
                    await self.cache_companies(self.companies_df)
            else:
                logger.info("No cached data found, downloading fresh company list...")
                await self.download_and_cache_companies()
                
        except Exception as e:
            logger.error(f"Error loading company list: {e}")
            self.symbols = []
            self.companies_df = pd.DataFrame()

    def _set_symbols_from_df(self):
        """DataFrameì—ì„œ ì¢…ëª©ì½”ë“œ ë¦¬ìŠ¤íŠ¸ ì„¤ì •"""
        if self.companies_df is None or 'Code' not in self.companies_df.columns:
            logger.error("DataFrame is not valid or 'Code' column is missing.")
            self.symbols = []
            return
        self.companies_df['Code'] = self.companies_df['Code'].astype(str).str.zfill(6)
        self.symbols = self.companies_df['Code'].tolist()

    async def download_and_cache_companies(self):
        """ì¢…ëª©ì½”ë“œ ë‹¤ìš´ë¡œë“œ, CSV ì €ì¥ ë° ìºì‹±"""
        try:
            logger.info("ğŸ“¡ Downloading KRX company list...")
            krx_list = fdr.StockListing('KRX')
            
            required_cols = ['Code', 'Name', 'Market']
            if not all(col in krx_list.columns for col in required_cols):
                 # 'Symbol' ì»¬ëŸ¼ì„ 'Code'ë¡œ ë³€ê²½
                if 'Symbol' in krx_list.columns and 'Code' not in krx_list.columns:
                    krx_list.rename(columns={'Symbol': 'Code'}, inplace=True)
                else:
                    raise ValueError(f"Required columns {required_cols} not found in KRX data.")
            
            cleaned_list = krx_list[required_cols + ['Sector']].copy()
            cleaned_list = cleaned_list.dropna(subset=['Code'])
            
            self.companies_df = cleaned_list
            self._set_symbols_from_df()
            
            cleaned_list.to_csv(self.csv_file, index=False, encoding='utf-8-sig')
            logger.info(f"ğŸ’¾ Company list saved to CSV: {self.csv_file}")
            
            if self.cache_manager:
                await self.cache_companies(cleaned_list)
            
            logger.info(f"âœ… Downloaded and cached {len(self.symbols)} companies")
            
        except Exception as e:
            logger.error(f"âŒ Error downloading company list: {e}")
            self.symbols = []
            self.companies_df = pd.DataFrame()

    async def get_cached_companies(self) -> Optional[pd.DataFrame]:
        """ìºì‹œì—ì„œ ì¢…ëª©ì½”ë“œ ì¡°íšŒ"""
        if not self.cache_manager or not self.cache_manager.redis_client: return None
        try:
            cached = self.cache_manager.redis_client.get(self.cache_key)
            if cached:
                return pd.DataFrame(json.loads(cached))
            return None
        except Exception as e:
            logger.warning(f"Cache get error for companies: {e}")
            return None

    async def cache_companies(self, df: pd.DataFrame):
        """ì¢…ëª©ì½”ë“œë¥¼ ìºì‹œì— ì €ì¥"""
        if not self.cache_manager or not self.cache_manager.redis_client: return
        try:
            data = df.to_dict('records')
            self.cache_manager.redis_client.setex(
                self.cache_key, self.cache_timeout, json.dumps(data)
            )
            logger.info(f"ğŸ“ Company list cached for {self.cache_timeout/3600:.1f} hours")
        except Exception as e:
            logger.warning(f"Cache set error for companies: {e}")

    def get_test_symbols(self, count: int = 10) -> List[str]:
        """í…ŒìŠ¤íŠ¸ìš© ì¢…ëª© ë°˜í™˜ (ì‹œê°€ì´ì•¡ ìƒìœ„)"""
        if self.companies_df is None or self.companies_df.empty:
            logger.warning("No company data available, returning default symbols.")
            return ['005930', '000660', '035420', '005935', '005490', 
                   '051910', '006400', '035720', '105560', '055550'][:count]
        
        kospi_companies = self.companies_df[self.companies_df['Market'] == 'KOSPI']
        selected = kospi_companies.head(count) if len(kospi_companies) >= count else self.companies_df.head(count)
        
        test_symbols = selected['Code'].tolist()
        logger.info(f"ğŸ” Selected {len(test_symbols)} test symbols: {test_symbols}")
        return test_symbols

class DataSourceManager:
    """ë°ì´í„° ì†ŒìŠ¤ ê´€ë¦¬"""
    
    def _validate_stock_data(self, symbol: str, row: pd.Series) -> bool:
        """ì£¼ê°€ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬"""
        prices = [row['Open'], row['High'], row['Low'], row['Close']]
        if any(p < 0 for p in prices) or row['Volume'] < 0:
            logger.warning(f"Negative value detected for {symbol}: {row.to_dict()}")
            return False
        if row['High'] < row['Low']:
            logger.warning(f"Invalid high-low range for {symbol}: High={row['High']}, Low={row['Low']}")
            return False
        return True

    async def _get_yfinance_data(self, symbol: str, start_date: str, end_date: str) -> List[StockPrice]:
        """Yahoo Finance ë°ì´í„° ìˆ˜ì§‘"""
        try:
            symbol_yf = f"{symbol}.KS"
            ticker = yf.Ticker(symbol_yf)
            hist = ticker.history(start=start_date, end=end_date)
            
            if hist.empty: return []
            
            valid_data = []
            for idx, row in hist.iterrows():
                if self._validate_stock_data(symbol, row):
                    valid_data.append(StockPrice(
                        symbol=symbol, timestamp=idx.to_pydatetime(),
                        open=float(row['Open']), high=float(row['High']),
                        low=float(row['Low']), close=float(row['Close']),
                        volume=int(row['Volume']), adj_close=float(row.get('Adj Close', row['Close']))
                    ))
            return valid_data
        except Exception as e:
            logger.error(f"Error fetching yfinance data for {symbol}: {e}")
            return []

    async def _get_fdr_data(self, symbol: str, start_date: str, end_date: str) -> List[StockPrice]:
        """FinanceDataReader ë°ì´í„° ìˆ˜ì§‘"""
        try:
            df = fdr.DataReader(symbol, start_date, end_date)
            if df.empty: return []
            
            valid_data = []
            for idx, row in df.iterrows():
                if self._validate_stock_data(symbol, row):
                    valid_data.append(StockPrice(
                        symbol=symbol, timestamp=idx.to_pydatetime(),
                        open=float(row['Open']), high=float(row['High']),
                        low=float(row['Low']), close=float(row['Close']),
                        volume=int(row['Volume']), adj_close=float(row.get('Adj Close', row['Close']))
                    ))
            return valid_data
        except Exception as e:
            logger.error(f"Error fetching FDR data for {symbol}: {e}")
            return []

class CacheManager:
    """Redis ìºì‹œ ê´€ë¦¬"""
    def __init__(self, host='localhost', port=6379, db=0):
        try:
            self.redis_client = redis.Redis(host=host, port=port, db=db, decode_responses=True)
            self.redis_client.ping()
            self.cache_timeout = 300  # 5ë¶„
            logger.info("âœ… Redis cache connected")
        except Exception as e:
            logger.warning(f"âš ï¸ Redis not available: {e}")
            self.redis_client = None

    async def get_cached_data(self, key: str) -> Optional[List[StockPrice]]:
        if not self.redis_client: return None
        try:
            cached = self.redis_client.get(key)
            if cached:
                data = json.loads(cached)
                return [StockPrice(**item) for item in data]
            return None
        except Exception as e:
            logger.error(f"Cache get error: {e}")
            return None

    async def cache_data(self, key: str, data: List[StockPrice]):
        if not self.redis_client: return
        try:
            serialized = [item.__dict__ for item in data]
            for item in serialized:
                item['timestamp'] = item['timestamp'].isoformat()
            self.redis_client.setex(key, self.cache_timeout, json.dumps(serialized))
        except Exception as e:
            logger.error(f"Cache set error: {e}")

class DatabaseManager:
    """ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬"""
    def __init__(self, db_url: str = "sqlite:///stock_data.db"):
        self.engine = create_engine(db_url)
        Base.metadata.create_all(self.engine)
        self.Session = sessionmaker(bind=self.engine)
    
    async def save_stock_data(self, data: List[StockPrice]):
        session = self.Session()
        try:
            for item in data:
                db_item = StockPriceDB(**item.__dict__)
                session.merge(db_item) # ì¤‘ë³µ ì‹œ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…
            session.commit()
        except Exception as e:
            session.rollback()
            logger.error(f"Database save error: {e}")
        finally:
            session.close()

class QuantDataManager:
    """í†µí•© ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ"""
    def __init__(self, db_url: str = "sqlite:///quant_data.db", force_update_companies: bool = False):
        self.data_source = DataSourceManager()
        self.cache_manager = CacheManager()
        self.db_manager = DatabaseManager(db_url)
        self.company_manager = CompanyListManager(self.cache_manager, force_update_companies)
        self.csv_output_dir = "stock_data_csv"
        os.makedirs(self.csv_output_dir, exist_ok=True)
    
    async def initialize_companies(self, force_update: bool = False):
        await self.company_manager.load_companies_from_cache_or_download(force_update)

    def get_test_symbols(self, count: int = 10) -> List[str]:
        return self.company_manager.get_test_symbols(count)
    
    async def get_stock_data(self, symbol: str, start_date: str, end_date: str, use_cache: bool = True) -> List[StockPrice]:
        cache_key = f"stock_data:{symbol}:{start_date}:{end_date}"
        if use_cache and self.cache_manager.redis_client:
            cached_data = await self.cache_manager.get_cached_data(cache_key)
            if cached_data:
                logger.info(f"ğŸ“‹ Cache hit for {symbol}")
                return cached_data
        
        logger.info(f"ğŸ“¡ Fetching data for {symbol} from API...")
        data = await self.data_source._get_yfinance_data(symbol, start_date, end_date)
        
        if not data: # yfinance ì‹¤íŒ¨ ì‹œ FDR ì‹œë„
            logger.warning(f"yfinance failed for {symbol}. Trying FinanceDataReader...")
            await asyncio.sleep(1) # ì§§ì€ ë”œë ˆì´
            data = await self.data_source._get_fdr_data(symbol, start_date, end_date)

        if data:
            if self.cache_manager.redis_client:
                await self.cache_manager.cache_data(cache_key, data)
            await self.db_manager.save_stock_data(data)
            logger.info(f"âœ… Data fetched for {symbol}: {len(data)} records")
        return data
    
    async def collect_test_data(self, test_symbols: List[str], start_date: str, end_date: str) -> Dict[str, List[StockPrice]]:
        logger.info(f"ğŸ”„ Starting data collection for {len(test_symbols)} symbols...")
        results = {}
        for i, symbol in enumerate(test_symbols, 1):
            try:
                logger.info(f"[{i}/{len(test_symbols)}] Processing {symbol}...")
                data = await self.get_stock_data(symbol, start_date, end_date)
                if data:
                    results[symbol] = data
                else:
                    logger.warning(f"  âŒ No data for {symbol}")
                await asyncio.sleep(1) # API ìš”ì²­ ê°„ ë”œë ˆì´
            except Exception as e:
                logger.error(f"  âŒ Error processing {symbol}: {e}")
        logger.info(f"ğŸ Test data collection completed: {sum(len(v) for v in results.values())} total records")
        return results

    async def save_data_to_csv(self, data_dict: Dict[str, List[StockPrice]], filename_prefix: str):
        if not data_dict:
            logger.warning("No data to save.")
            return None
            
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_filename = f"{self.csv_output_dir}/{filename_prefix}_{timestamp}.csv"
        
        all_records = []
        for symbol, stock_data in data_dict.items():
            company_name = self._get_company_name(symbol)
            for price in stock_data:
                record = price.__dict__
                record['company_name'] = company_name
                record['timestamp'] = price.timestamp.strftime('%Y-%m-%d %H:%M:%S')
                all_records.append(record)
        
        if not all_records:
            logger.warning("No valid records to save in CSV.")
            return None

        df_all = pd.DataFrame(all_records)
        df_all.to_csv(csv_filename, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ’¾ All data saved to CSV: {csv_filename}")
        return csv_filename

    def _get_company_name(self, symbol: str) -> str:
        """ì¢…ëª© ì½”ë“œë¡œ íšŒì‚¬ ì´ë¦„ ì¡°íšŒ"""
        if self.company_manager.companies_df is not None:
            info = self.company_manager.companies_df[self.company_manager.companies_df['Code'] == symbol]
            if not info.empty:
                return info.iloc[0]['Name']
        return "Unknown"

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì‚¬ìš©ì ì…ë ¥ ê¸°ë°˜"""
    
    # 1. ì¢…ëª© ê°œìˆ˜ ì…ë ¥ ë°›ê¸°
    while True:
        try:
            count_str = input("1. ë‹¤ìš´ë¡œë“œí•  ì¢…ëª© ê°¯ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 10): ")
            if not count_str:
                test_count = 10
                break
            test_count = int(count_str)
            if test_count > 0:
                break
            else:
                logger.warning("âŒ 1 ì´ìƒì˜ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        except ValueError:
            logger.warning("âŒ ìœ íš¨í•œ ìˆ«ìê°€ ì•„ë‹™ë‹ˆë‹¤.")

    # 2. ì—°ë„ ì…ë ¥ ë°›ê¸°
    while True:
        try:
            year_str = input(f"2. ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•  ì—°ë„ë¥¼ ì…ë ¥í•˜ì„¸ìš” (YYYY, ì˜ˆ: {datetime.now().year - 1}): ")
            year = int(year_str)
            if 1990 < year <= datetime.now().year:
                start_date = f"{year}-01-01"
                end_date = f"{year}-12-31"
                break
            else:
                logger.warning(f"âŒ 1990ë…„ê³¼ {datetime.now().year}ë…„ ì‚¬ì´ì˜ ìœ íš¨í•œ ì—°ë„ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        except ValueError:
            logger.warning("âŒ ìœ íš¨í•œ ì—°ë„ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
            
    logger.info("ğŸš€ Initializing Quant Data Management System...")
    quant_manager = QuantDataManager()
    
    try:
        await quant_manager.initialize_companies()
        test_symbols = quant_manager.get_test_symbols(test_count)
        
        if not test_symbols:
            logger.error("No symbols to process. Exiting.")
            return
            
        collected_data = await quant_manager.collect_test_data(test_symbols, start_date, end_date)
        
        if collected_data:
            filename_prefix = f"stock_data_{len(test_symbols)}symbols_{year}"
            await quant_manager.save_data_to_csv(collected_data, filename_prefix)
        else:
            logger.warning("âš ï¸ No data collected.")
            
    except Exception as e:
        logger.error(f"ğŸ’¥ An unexpected error occurred during execution: {e}")
        import traceback
        traceback.print_exc()
    
    logger.info("ğŸ Quant Data Management System finished")

if __name__ == "__main__":
    asyncio.run(main())