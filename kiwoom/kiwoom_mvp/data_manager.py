"""
file: main.py
í‚¤ì›€ì¦ê¶Œ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ê¸° - ë°ì´í„° ê´€ë¦¬ì
Phase 1 MVP

CSV íŒŒì¼ ì €ì¥/ë¡œë“œ, ë°ì´í„° ê²€ì¦, íŒŒì¼ ê´€ë¦¬ ê¸°ëŠ¥
ìˆ˜ì§‘ëœ ì£¼ì‹ ë°ì´í„°ì˜ ì €ì¥ì†Œ ì—­í• 
"""

import os
import logging
import shutil
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import pandas as pd
import numpy as np
from config import get_config

class DataManager:
    """ì£¼ì‹ ë°ì´í„° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.config = get_config()
        self.logger = logging.getLogger(__name__)
        
        # íŒŒì¼ ê²½ë¡œ ì„¤ì •
        self.csv_base_path = self.config.CSV_SAVE_PATH
        self.daily_path = self.csv_base_path / 'daily'
        self.minute_path = self.csv_base_path / 'minute'
        self.backup_path = self.csv_base_path / 'backup'
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_directories()
        
        # ë°ì´í„° ìºì‹œ
        self.data_cache: Dict[str, pd.DataFrame] = {}
        self.cache_timestamps: Dict[str, datetime] = {}
        self.cache_max_age = timedelta(minutes=30)  # 30ë¶„ ìºì‹œ
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        directories = [
            self.csv_base_path,
            self.daily_path,
            self.minute_path,
            self.backup_path
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
        
        self.logger.debug(f"ğŸ“ ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: {len(directories)}ê°œ")
    
    def save_daily_data(self, stock_code: str, stock_name: str, data: pd.DataFrame,
                       start_date: str = None, end_date: str = None) -> Optional[str]:
        """ì¼ë´‰ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        try:
            if data is None or data.empty:
                self.logger.warning(f"âš ï¸ {stock_code} ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ë‚ ì§œ ë²”ìœ„ ìë™ ê³„ì‚° (ë°ì´í„°ì—ì„œ ì¶”ì¶œ)
            if start_date is None or end_date is None:
                dates = data['ë‚ ì§œ'].tolist()
                start_date = min(dates)
                end_date = max(dates)
            
            # íŒŒì¼ëª… ìƒì„±
            filename = self.config.get_csv_filename(
                stock_code, stock_name, start_date, end_date, 'daily'
            )
            filepath = self.daily_path / filename
            
            # ë°ì´í„° ê²€ì¦
            validated_data = self._validate_daily_data(data)
            if validated_data is None:
                self.logger.error(f"âŒ {stock_code} ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨")
                return None
            
            # ê¸°ì¡´ íŒŒì¼ ë°±ì—… (ë®ì–´ì“°ê¸° ì „)
            if filepath.exists():
                self._backup_file(filepath)
            
            # CSV ì €ì¥
            validated_data.to_csv(
                filepath,
                index=False,
                encoding=self.config.CSV_ENCODING
            )
            
            # ì €ì¥ í›„ ê²€ì¦
            if self._verify_saved_file(filepath):
                self.logger.info(f"âœ… {stock_name}({stock_code}) ì¼ë´‰ ë°ì´í„° ì €ì¥: {filename}")
                
                # ìºì‹œ ì—…ë°ì´íŠ¸
                cache_key = f"daily_{stock_code}"
                self.data_cache[cache_key] = validated_data.copy()
                self.cache_timestamps[cache_key] = datetime.now()
                
                return str(filepath)
            else:
                self.logger.error(f"âŒ {stock_code} íŒŒì¼ ì €ì¥ ê²€ì¦ ì‹¤íŒ¨")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ {stock_code} ì¼ë´‰ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return None
    
    def load_daily_data(self, stock_code: str, use_cache: bool = True) -> Optional[pd.DataFrame]:
        """ì¼ë´‰ ë°ì´í„° ë¡œë“œ"""
        try:
            # ìºì‹œ í™•ì¸
            if use_cache:
                cached_data = self._get_cached_data(f"daily_{stock_code}")
                if cached_data is not None:
                    self.logger.debug(f"ğŸ“Š {stock_code} ìºì‹œì—ì„œ ì¼ë´‰ ë°ì´í„° ë¡œë“œ")
                    return cached_data
            
            # ìµœì‹  íŒŒì¼ ì°¾ê¸°
            latest_file = self._find_latest_daily_file(stock_code)
            if not latest_file:
                self.logger.warning(f"âš ï¸ {stock_code} ì¼ë´‰ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # CSV íŒŒì¼ ë¡œë“œ
            data = pd.read_csv(latest_file, encoding=self.config.CSV_ENCODING)
            
            # ë°ì´í„° ê²€ì¦
            validated_data = self._validate_daily_data(data)
            if validated_data is None:
                self.logger.error(f"âŒ {stock_code} ë¡œë“œëœ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨")
                return None
            
            # ìºì‹œ ì €ì¥
            cache_key = f"daily_{stock_code}"
            self.data_cache[cache_key] = validated_data.copy()
            self.cache_timestamps[cache_key] = datetime.now()
            
            self.logger.info(f"âœ… {stock_code} ì¼ë´‰ ë°ì´í„° ë¡œë“œ: {len(validated_data)}ê°œ")
            return validated_data
            
        except Exception as e:
            self.logger.error(f"âŒ {stock_code} ì¼ë´‰ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _validate_daily_data(self, data: pd.DataFrame) -> Optional[pd.DataFrame]:
        """ì¼ë´‰ ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬"""
        if data is None or data.empty:
            return None
        
        try:
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['ë‚ ì§œ', 'ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ê±°ë˜ëŸ‰']
            missing_columns = [col for col in required_columns if col not in data.columns]
            
            if missing_columns:
                self.logger.error(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
                return None
            
            # ë°ì´í„° ë³µì‚¬
            validated_data = data.copy()
            
            # ë‚ ì§œ í˜•ì‹ í†µì¼ (YYYY-MM-DD ë˜ëŠ” YYYYMMDD)
            validated_data['ë‚ ì§œ'] = validated_data['ë‚ ì§œ'].astype(str)
            validated_data['ë‚ ì§œ'] = validated_data['ë‚ ì§œ'].apply(self._normalize_date_format)
            
            # ìˆ«ìí˜• ì»¬ëŸ¼ ë³€í™˜
            numeric_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ê±°ë˜ëŸ‰']
            for col in numeric_columns:
                validated_data[col] = pd.to_numeric(validated_data[col], errors='coerce')
            
            # ê²°ì¸¡ê°’ í™•ì¸
            null_counts = validated_data[required_columns].isnull().sum()
            if null_counts.sum() > 0:
                self.logger.warning(f"âš ï¸ ê²°ì¸¡ê°’ ë°œê²¬: {null_counts[null_counts > 0].to_dict()}")
                
                # ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°
                validated_data = validated_data.dropna(subset=required_columns)
            
            # ë°ì´í„° ì •í•©ì„± ê²€ì¦
            invalid_rows = []
            
            # ê°€ê²© ë°ì´í„° ê²€ì¦ (ì‹œê°€, ê³ ê°€, ì €ê°€, ì¢…ê°€ > 0)
            price_cols = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€']
            for idx, row in validated_data.iterrows():
                # ìŒìˆ˜ ê°€ê²© í™•ì¸
                if any(row[col] <= 0 for col in price_cols):
                    invalid_rows.append(idx)
                    continue
                
                # ê³ ê°€ >= ì €ê°€ í™•ì¸
                if row['ê³ ê°€'] < row['ì €ê°€']:
                    invalid_rows.append(idx)
                    continue
                
                # ì‹œê°€, ì¢…ê°€ê°€ ê³ ê°€-ì €ê°€ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
                if not (row['ì €ê°€'] <= row['ì‹œê°€'] <= row['ê³ ê°€']):
                    invalid_rows.append(idx)
                    continue
                
                if not (row['ì €ê°€'] <= row['ì¢…ê°€'] <= row['ê³ ê°€']):
                    invalid_rows.append(idx)
                    continue
                
                # ê±°ë˜ëŸ‰ ê²€ì¦ (ìŒìˆ˜ ë¶ˆê°€)
                if row['ê±°ë˜ëŸ‰'] < 0:
                    invalid_rows.append(idx)
                    continue
            
            # ì˜ëª»ëœ ë°ì´í„° ì œê±°
            if invalid_rows:
                self.logger.warning(f"âš ï¸ ì˜ëª»ëœ ë°ì´í„° {len(invalid_rows)}ê°œ ì œê±°")
                validated_data = validated_data.drop(invalid_rows)
            
            # ë‚ ì§œìˆœ ì •ë ¬ (ê³¼ê±° â†’ í˜„ì¬)
            validated_data = validated_data.sort_values('ë‚ ì§œ').reset_index(drop=True)
            
            # ì¤‘ë³µ ë‚ ì§œ ì œê±° (ìµœì‹  ë°ì´í„° ìœ ì§€)
            before_count = len(validated_data)
            validated_data = validated_data.drop_duplicates(subset=['ë‚ ì§œ'], keep='last').reset_index(drop=True)
            after_count = len(validated_data)
            
            if before_count != after_count:
                self.logger.warning(f"âš ï¸ ì¤‘ë³µ ë‚ ì§œ {before_count - after_count}ê°œ ì œê±°")
            
            if len(validated_data) == 0:
                self.logger.error("âŒ ê²€ì¦ í›„ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            return validated_data
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _normalize_date_format(self, date_str: str) -> str:
        """ë‚ ì§œ í˜•ì‹ ì •ê·œí™” (YYYYMMDD)"""
        try:
            # ê³µë°± ì œê±°
            date_str = str(date_str).strip()
            
            # ì´ë¯¸ YYYYMMDD í˜•ì‹ì¸ ê²½ìš°
            if len(date_str) == 8 and date_str.isdigit():
                return date_str
            
            # YYYY-MM-DD í˜•ì‹ì¸ ê²½ìš°
            if len(date_str) == 10 and '-' in date_str:
                return date_str.replace('-', '')
            
            # ê¸°íƒ€ í˜•ì‹ ì‹œë„
            from datetime import datetime
            parsed_date = datetime.strptime(date_str, '%Y%m%d')
            return parsed_date.strftime('%Y%m%d')
            
        except:
            # íŒŒì‹± ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
            return date_str
    
    def _find_latest_daily_file(self, stock_code: str) -> Optional[Path]:
        """íŠ¹ì • ì¢…ëª©ì˜ ìµœì‹  ì¼ë´‰ íŒŒì¼ ì°¾ê¸°"""
        try:
            # í•´ë‹¹ ì¢…ëª©ì˜ ëª¨ë“  ì¼ë´‰ íŒŒì¼ ê²€ìƒ‰
            pattern = f"{stock_code}_*_daily_*.csv"
            files = list(self.daily_path.glob(pattern))
            
            if not files:
                return None
            
            # íŒŒì¼ ìƒì„± ì‹œê°„ ê¸°ì¤€ ìµœì‹  íŒŒì¼ ì„ íƒ
            latest_file = max(files, key=lambda f: f.stat().st_mtime)
            
            self.logger.debug(f"ğŸ“ {stock_code} ìµœì‹  ì¼ë´‰ íŒŒì¼: {latest_file.name}")
            return latest_file
            
        except Exception as e:
            self.logger.error(f"âŒ {stock_code} ìµœì‹  íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def _backup_file(self, filepath: Path):
        """íŒŒì¼ ë°±ì—…"""
        try:
            if not filepath.exists():
                return
            
            # ë°±ì—… íŒŒì¼ëª… ìƒì„± (ì›ë³¸ëª…_YYYYMMDD_HHMMSS.csv)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_name = f"{filepath.stem}_{timestamp}{filepath.suffix}"
            backup_filepath = self.backup_path / backup_name
            
            # ë°±ì—… ë³µì‚¬
            shutil.copy2(filepath, backup_filepath)
            
            self.logger.debug(f"ğŸ’¾ íŒŒì¼ ë°±ì—…: {backup_name}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ íŒŒì¼ ë°±ì—… ì‹¤íŒ¨: {e}")
    
    def _verify_saved_file(self, filepath: Path) -> bool:
        """ì €ì¥ëœ íŒŒì¼ ê²€ì¦"""
        try:
            if not filepath.exists():
                return False
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            if filepath.stat().st_size == 0:
                return False
            
            # CSV íŒŒì¼ ë¡œë“œ í…ŒìŠ¤íŠ¸
            test_data = pd.read_csv(filepath, encoding=self.config.CSV_ENCODING, nrows=1)
            if test_data.empty:
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def _get_cached_data(self, cache_key: str) -> Optional[pd.DataFrame]:
        """ìºì‹œëœ ë°ì´í„° ì¡°íšŒ"""
        if cache_key not in self.data_cache:
            return None
        
        # ìºì‹œ ë§Œë£Œ í™•ì¸
        if cache_key in self.cache_timestamps:
            cache_age = datetime.now() - self.cache_timestamps[cache_key]
            if cache_age > self.cache_max_age:
                # ë§Œë£Œëœ ìºì‹œ ì œê±°
                del self.data_cache[cache_key]
                del self.cache_timestamps[cache_key]
                return None
        
        return self.data_cache[cache_key].copy()
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.data_cache.clear()
        self.cache_timestamps.clear()
        self.logger.info("ğŸ§¹ ë°ì´í„° ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_stock_file_list(self, stock_code: str = None) -> List[Dict]:
        """ì¢…ëª©ë³„ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            file_list = []
            
            # ê²€ìƒ‰ íŒ¨í„´ ì„¤ì •
            if stock_code:
                patterns = [f"{stock_code}_*_daily_*.csv"]
            else:
                patterns = ["*_daily_*.csv"]
            
            for pattern in patterns:
                files = list(self.daily_path.glob(pattern))
                
                for file_path in files:
                    try:
                        # íŒŒì¼ëª…ì—ì„œ ì •ë³´ ì¶”ì¶œ
                        parts = file_path.stem.split('_')
                        if len(parts) >= 5:
                            code = parts[0]
                            name = parts[1]
                            data_type = parts[2]
                            start_date = parts[3]
                            end_date = parts[4]
                            
                            # íŒŒì¼ ì •ë³´ êµ¬ì„±
                            stat = file_path.stat()
                            file_info = {
                                'code': code,
                                'name': name,
                                'data_type': data_type,
                                'start_date': start_date,
                                'end_date': end_date,
                                'filepath': str(file_path),
                                'filename': file_path.name,
                                'size': stat.st_size,
                                'created': datetime.fromtimestamp(stat.st_ctime),
                                'modified': datetime.fromtimestamp(stat.st_mtime)
                            }
                            file_list.append(file_info)
                    except:
                        continue
            
            # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ìµœì‹ ìˆœ ì •ë ¬
            file_list.sort(key=lambda x: x['modified'], reverse=True)
            
            return file_list
            
        except Exception as e:
            self.logger.error(f"âŒ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def delete_stock_data(self, stock_code: str, backup: bool = True) -> bool:
        """ì¢…ëª© ë°ì´í„° íŒŒì¼ ì‚­ì œ"""
        try:
            deleted_count = 0
            
            # í•´ë‹¹ ì¢…ëª©ì˜ ëª¨ë“  íŒŒì¼ ì°¾ê¸°
            patterns = [
                f"{stock_code}_*_daily_*.csv",
                f"{stock_code}_*_minute_*.csv"
            ]
            
            for pattern in patterns:
                files = list(self.csv_base_path.rglob(pattern))
                
                for file_path in files:
                    try:
                        if backup:
                            self._backup_file(file_path)
                        
                        file_path.unlink()  # íŒŒì¼ ì‚­ì œ
                        deleted_count += 1
                        self.logger.debug(f"ğŸ—‘ï¸ íŒŒì¼ ì‚­ì œ: {file_path.name}")
                        
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({file_path.name}): {e}")
            
            # ìºì‹œì—ì„œë„ ì œê±°
            cache_keys_to_remove = [key for key in self.data_cache.keys() if stock_code in key]
            for key in cache_keys_to_remove:
                del self.data_cache[key]
                if key in self.cache_timestamps:
                    del self.cache_timestamps[key]
            
            if deleted_count > 0:
                self.logger.info(f"âœ… {stock_code} ë°ì´í„° íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {deleted_count}ê°œ")
                return True
            else:
                self.logger.warning(f"âš ï¸ {stock_code} ì‚­ì œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ {stock_code} ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup_old_backups(self, days_to_keep: int = 30):
        """ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)
            deleted_count = 0
            
            backup_files = list(self.backup_path.glob("*.csv"))
            
            for backup_file in backup_files:
                try:
                    file_time = datetime.fromtimestamp(backup_file.stat().st_mtime)
                    
                    if file_time < cutoff_date:
                        backup_file.unlink()
                        deleted_count += 1
                        self.logger.debug(f"ğŸ—‘ï¸ ë°±ì—… íŒŒì¼ ì •ë¦¬: {backup_file.name}")
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ ë°±ì—… íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ({backup_file.name}): {e}")
            
            if deleted_count > 0:
                self.logger.info(f"ğŸ§¹ ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬: {deleted_count}ê°œ ì‚­ì œ")
            
        except Exception as e:
            self.logger.error(f"âŒ ë°±ì—… íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def get_data_summary(self) -> Dict:
        """ë°ì´í„° ìš”ì•½ ì •ë³´"""
        try:
            summary = {
                'total_files': 0,
                'total_size': 0,
                'stocks': {},
                'date_range': {},
                'cache_status': {
                    'cached_items': len(self.data_cache),
                    'cache_size': sum(df.memory_usage(deep=True).sum() for df in self.data_cache.values()) if self.data_cache else 0
                }
            }
            
            # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
            all_files = self.get_stock_file_list()
            summary['total_files'] = len(all_files)
            
            for file_info in all_files:
                code = file_info['code']
                
                # ì¢…ëª©ë³„ í†µê³„
                if code not in summary['stocks']:
                    summary['stocks'][code] = {
                        'name': file_info['name'],
                        'file_count': 0,
                        'total_size': 0,
                        'latest_data': None
                    }
                
                summary['stocks'][code]['file_count'] += 1
                summary['stocks'][code]['total_size'] += file_info['size']
                summary['total_size'] += file_info['size']
                
                # ìµœì‹  ë°ì´í„° ì¼ì
                if summary['stocks'][code]['latest_data'] is None or file_info['end_date'] > summary['stocks'][code]['latest_data']:
                    summary['stocks'][code]['latest_data'] = file_info['end_date']
            
            return summary
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return {}
    
    def export_to_excel(self, stock_code: str, output_path: str = None) -> Optional[str]:
        """ë°ì´í„°ë¥¼ Excel íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            # ë°ì´í„° ë¡œë“œ
            data = self.load_daily_data(stock_code)
            if data is None or data.empty:
                self.logger.warning(f"âš ï¸ {stock_code} ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            if output_path is None:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f"{stock_code}_ì£¼ì‹ë°ì´í„°_{timestamp}.xlsx"
                output_path = self.csv_base_path / filename
            
            # Excel íŒŒì¼ë¡œ ì €ì¥
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                data.to_excel(writer, sheet_name='ì¼ë´‰ë°ì´í„°', index=False)
                
                # ìš”ì•½ ì •ë³´ ì‹œíŠ¸ ì¶”ê°€
                summary_data = {
                    'í•­ëª©': ['ì¢…ëª©ì½”ë“œ', 'ë°ì´í„° ê°œìˆ˜', 'ê¸°ê°„ ì‹œì‘', 'ê¸°ê°„ ì¢…ë£Œ', 'ìµœê³ ê°€', 'ìµœì €ê°€'],
                    'ê°’': [
                        stock_code,
                        len(data),
                        data['ë‚ ì§œ'].min(),
                        data['ë‚ ì§œ'].max(),
                        data['ê³ ê°€'].max(),
                        data['ì €ê°€'].min()
                    ]
                }
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='ìš”ì•½ì •ë³´', index=False)
            
            self.logger.info(f"âœ… {stock_code} Excel ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
            return str(output_path)
            
        except Exception as e:
            self.logger.error(f"âŒ {stock_code} Excel ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return None

# ì „ì—­ ë°ì´í„° ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_data_manager = None

def get_data_manager() -> DataManager:
    """ë°ì´í„° ë§¤ë‹ˆì € ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _data_manager
    if _data_manager is None:
        _data_manager = DataManager()
    return _data_manager

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    import logging
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸ§ª ë°ì´í„° ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print("=" * 50)
    
    # ë°ì´í„° ë§¤ë‹ˆì € ìƒì„±
    manager = get_data_manager()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_data = pd.DataFrame({
        'ë‚ ì§œ': ['20240801', '20240802', '20240803'],
        'ì‹œê°€': [84000, 84500, 85000],
        'ê³ ê°€': [85000, 85500, 86000],
        'ì €ê°€': [83500, 84000, 84500],
        'ì¢…ê°€': [84500, 85000, 85500],
        'ê±°ë˜ëŸ‰': [1000000, 1100000, 1200000]
    })
    
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°:")
    print(test_data)
    
    # ì €ì¥ í…ŒìŠ¤íŠ¸
    print("\nğŸ’¾ ì €ì¥ í…ŒìŠ¤íŠ¸:")
    save_path = manager.save_daily_data('000000', 'í…ŒìŠ¤íŠ¸ì¢…ëª©', test_data)
    if save_path:
        print(f"âœ… ì €ì¥ ì„±ê³µ: {save_path}")
    else:
        print("âŒ ì €ì¥ ì‹¤íŒ¨")
    
    # ë¡œë“œ í…ŒìŠ¤íŠ¸
    print("\nğŸ“‚ ë¡œë“œ í…ŒìŠ¤íŠ¸:")
    loaded_data = manager.load_daily_data('000000')
    if loaded_data is not None:
        print(f"âœ… ë¡œë“œ ì„±ê³µ: {len(loaded_data)}ê°œ ë ˆì½”ë“œ")
        print(loaded_data.head())
    else:
        print("âŒ ë¡œë“œ ì‹¤íŒ¨")
    
    # ë°ì´í„° ìš”ì•½
    print("\nğŸ“ˆ ë°ì´í„° ìš”ì•½:")
    summary = manager.get_data_summary()
    print(f"   - ì´ íŒŒì¼ ìˆ˜: {summary.get('total_files', 0)}ê°œ")
    print(f"   - ì´ í¬ê¸°: {summary.get('total_size', 0):,} bytes")
    print(f"   - ìºì‹œëœ í•­ëª©: {summary.get('cache_status', {}).get('cached_items', 0)}ê°œ")
    
    print("\n" + "=" * 50)
    print("ğŸ‰ ë°ì´í„° ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")